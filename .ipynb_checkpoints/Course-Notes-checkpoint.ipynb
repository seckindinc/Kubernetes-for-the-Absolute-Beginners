{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Container Orchestration__\n",
    "\n",
    "    Automatically deploying and managing containers is known as container orchestration.\n",
    "\n",
    "\n",
    "        - Kubernetes Advantage\n",
    "\n",
    "    Application is highly available as, hardware failures will not bring the applications down because we have multiple instances of the application running on different nodes.\n",
    "    User traffic is load balanced across various containers. When demand increases, increases the instances seamlessly. \n",
    "\n",
    "\n",
    "## __Kubernetes Architecture__ \n",
    "\n",
    "        - Nodes\n",
    "    \n",
    "            Node is a machine which can be physical or virtual on which Kubernetes is installed. Nodes is a worker machine where containers will be launched by Kubernetes. When the node that application runs fails, application goes down. So we need more than one node. \n",
    "\n",
    "        - Cluster\n",
    "\n",
    "            A cluster is a set of nodes grouped together. This way even if one node fails, application is still available through other nodes. Moreover multiple nodes help sharing load as well.\n",
    "\n",
    "        - Master\n",
    "\n",
    "            Master is another node in Kubernetes cluster and configured as a Master. Master watches all over the nodes and is responsible for the actual orchestration of the containers on the worker nodes. \n",
    "\n",
    "## __Kubernetes Components__\n",
    "\n",
    "        - API Server__: API Server acts like the front-end of Kubernetes. Any third party application talks to API Server to interact with Kubernetes Cluster.\n",
    "\n",
    "        - Etcd service: Distributed key-value store used by Kubernetes to store all data to manage the cluster. For example when we have multiple master and nodes, all the information related to them is stored at etcd.\n",
    "\n",
    "        - Kubelet: Kubelet is the agent that runs in each node in the cluster. Agent is responsible for the containers running as nodes expected. \n",
    "\n",
    "        - Container Runtime: Underlying software to run containers. In most of the cases it is Docker. \n",
    "\n",
    "        - Controller: It is the brain behind the orchestration. They are responsible for noticing nodes, containers or end points goes down. It makes decisions to make new containers in such cases. \n",
    "\n",
    "        - Scheduler: Responsible for distributing work on containers across multiple nodes. It looks for newly created containers and assigns them to nodes. \n",
    "\n",
    "## __Master vs Worker Nodes__\n",
    "\n",
    "    Worker node is known where the containers are hosted, for example Docker containers. Worker node has the kubelet agent for the health check that master node is looking for.\n",
    "    \n",
    "    Master node has kube-apiserver, etcd, controller and scheduler. All the information gathered is stored at etcd on Master Node. \n",
    "\n",
    "    Kubectl\n",
    "\n",
    "        Kubectl is Kubernetes command line tool. \n",
    "        \n",
    "        In order to deploy an application on the cluster;\n",
    "\n",
    "            kubectl run hello-minikube\n",
    "\n",
    "        In order to get cluster information;\n",
    "\n",
    "            kubectl cluster-info\n",
    "\n",
    "        In order to get the list of the nodes on the cluster;\n",
    "\n",
    "            kubectl get nodes\n",
    "\n",
    "\n",
    "        Lab and Exercises:\n",
    "\n",
    "        Q: What is the flavour and version of OS on which the Kubernetes nodes are running?\n",
    "        A: kubectl get nodes -o wide\n",
    "\n",
    "\n",
    "## __How to setup Kubernetes?__\n",
    "\n",
    "        There are lots of ways to setup Kuberentes. We can setup it up ourselves locally on our laptops or virtual machines using solutions like Minikube and Kubeadmin. Minikube is a tool used to setup a single instance of Kubernetes in an All-in-one setup and kubeadmin is a tool used to configure kubernetes in a multi-node setup. \n",
    "\n",
    "        There are also hosted solutions available for setting up kubernetes in a cloud environment such as GCP and AWS. \n",
    "\n",
    "        Minikube\n",
    "\n",
    "            Minikube bundles all the Kubernetes components into a single image providing us a pre-configured single node kubernetes cluster so we can get started in a matter of minutes. \n",
    "\n",
    "            The whole bundle is packaged into an ISO image and is available online for download. \n",
    "            \n",
    "## __PODS__\n",
    "\n",
    "        Kubernetes does not deploy containers directly on the worker nodes. The containers are encapsulated into a Kubernetes object known as PODs. A POD is a single instance of an application. A POD is the smallest object that you can create in kubernetes. \n",
    "\n",
    "\n",
    "        When we need to add additional instances of our application to share the load, we create a new POD altogether with a new instance of the same application. When our current node has no sufficient capacity, we can always deploy additional PODs on a new node in the cluster. \n",
    "        \n",
    "        PODs usually have a one-to-one relationship with containers running our application. To scale up we create new PODs and to scale down we delete PODs. We do not add additional containers to an existing POD to scale our application. \n",
    "\n",
    "        Each pod has its internal IP address.\n",
    "\n",
    "\n",
    "        - Multiple Containers in a POD\n",
    "\n",
    "                A single POD can have multiple containers, except for the fact that they are usually not multiple containers of the same kind. These containers can be complementary to each other. These containers can share the same network and volume.\n",
    "\n",
    "\n",
    "        To create a pod;\n",
    "\n",
    "            kubectl run nginx --image nginx\n",
    "\n",
    "        To get list of pods in our cluster;\n",
    "\n",
    "            kubectl get pods -o wide\n",
    "\n",
    "        To get detailed information about pods;\n",
    "\n",
    "            kubectl describe pods\n",
    "\n",
    "## __Introduction to YAML__\n",
    "\n",
    "        YAML file represents configuration data. In the YAML file each property should have equal length of spaces.\n",
    "\n",
    "                Servers:\n",
    "\n",
    "                    name: ABC\n",
    "                    owner: John\n",
    "                    created: 123\n",
    "                    status: active\n",
    "\n",
    "        - PODs with YAML\n",
    "\n",
    "            Each yml file must have these required fields. \n",
    "\n",
    "            apiVersion: Kubernetes version to create the object. May have different combinations regarding to our needs;\n",
    "\n",
    "            Kind Version\n",
    "            POD v1\n",
    "            Service v1\n",
    "            ReplicaSet apps/v1\n",
    "            Deployment apps/v1\n",
    "\n",
    "            - kind: Type of object we are trying to create. In this case it is a POD.\n",
    "\n",
    "            - metadata: Data about the object. We can define any key - value pairs to label the pod. For example;\n",
    "\n",
    "                metadata:\n",
    "                  name: myapp-pod\n",
    "                  labels:\n",
    "                    app: myapp\n",
    "                                   type: front-end\n",
    "\n",
    "            - spec: Depending on the object we are creating, we are providing additional information to Kubernetes. If we have additional containers in the same pod we need to define them within ‘containers’ section with each one with specific name and image. \n",
    "\n",
    "                spec:\n",
    "                 containers:\n",
    "                    - name:nginx-container\n",
    "                      image: nginx\n",
    "\n",
    "        To create a new pod from a yaml file;\n",
    "\n",
    "            kubectl create -f pod-definition.yml\n",
    "\n",
    "        To delete a pod;\n",
    "\n",
    "            kubectl delete deployment nginx\n",
    "\n",
    "        At Pycharm we can create yml file and use plugins to help us to check the format automatically. \n",
    "        Yaml example, pod definition;\n",
    "\n",
    "        apiVersion: v1\n",
    "        kind: Pod\n",
    "        metadata:\n",
    "          name: postgres\n",
    "          labels:\n",
    "            tier: db-tier\n",
    "        spec:\n",
    "          containers:\n",
    "            - name: postgres\n",
    "              image: postgres\n",
    "              env: \n",
    "                - name: POSTGRES_PASSWORD\n",
    "                  value: mysecretpassword\n",
    "\n",
    "    Lab & Exercises:\n",
    "\n",
    "        To get how many containers exits and runs on a pod; kubectl edit pod redis\n",
    "\n",
    "            kubectl get pods\n",
    "\n",
    "            NAME            READY   STATUS             RESTARTS   AGE\n",
    "            newpods-htv8n   1/1     Running            0          11m\n",
    "            newpods-pnbhj   1/1     Running            0          11m\n",
    "            newpods-s8bk7   1/1     Running            0          11m\n",
    "            nginx           1/1     Running            0          11m\n",
    "            webapp          1/2     ImagePullBackOff   0          4m29s\n",
    "\n",
    "        The Ready section defines running containers on the pod / total containers on the pod.\n",
    "\n",
    "        To edit an existing pods yaml file;\n",
    "\n",
    "        kubectl edit pod redis\n",
    "\n",
    "## __Replication Controller & ReplicaSets__\n",
    "\n",
    "\n",
    "    The replication controller helps us run multiple instances of a single POD in the kubernetes cluster thus providing high availability. So does that mean you can’t use a replication controller if you plan to have a single POD? No! Even if you have a single POD, the replication controller can help by automatically bringing up a new POD when the existing one fails. Thus the replication controller ensures that the specified number of PODs are running at all times.\n",
    "\n",
    "    Another reason we need a replication controller is to create multiple PODs to share the load across them. When the number of users increases we deploy additional POD to balance the load across the pods. If the demand further increases and If we were to run out of resources on the first node, we could deploy additional PODs across other nodes in the cluster. The replication controller spans across multiple nodes in the cluster. It helps us balance the load across multiple PODs on different nodes as well as scale our application when the demand increases.\n",
    "\n",
    "    Replication controllers and replica sets refer to the same thing but they have minor differences. Replica sets is the new technology.\n",
    "\n",
    "    Replica controller definition yaml example;\n",
    "\n",
    "    rc-definition.yml\n",
    "\n",
    "    apiVersion: v1\n",
    "    kind: ReplicationController\n",
    "    metadata: \n",
    "        name:myapp-rc\n",
    "        labels:\n",
    "            app: myapp\n",
    "            type: front-end\n",
    "    spec: #At this specification we will introduce the pod specifications to be replicated.\n",
    "        - template: \n",
    "            metadata: \n",
    "            name:myapp-pod\n",
    "            labels:\n",
    "                app: myapp\n",
    "            type: front-end\n",
    "        spec:\n",
    "            containers:\n",
    "    name: nginx-container\n",
    "    image: nginx\n",
    "\n",
    "        replicas: 3\n",
    "\n",
    "        To create the replication controller;\n",
    "\n",
    "            kubectl create -f rc-definition.yml\n",
    "\n",
    "        To view the replication controller;\n",
    "\n",
    "            kubectl get repliationcontroller\n",
    "\n",
    "        To view the pods were created by replication controller;\n",
    "\n",
    "            kubectl get pods\n",
    "\n",
    "        Replica set definition yaml example;\n",
    "\n",
    "    replicationset-definitions.yml\n",
    "\n",
    "    apiVersion: apps/v1\n",
    "    kind: ReplicaSet\n",
    "    metadata: \n",
    "        name:myapp-rc\n",
    "        labels:\n",
    "            app: myapp\n",
    "            type: front-end\n",
    "    spec: #At this specification we will introduce the pod specifications to be replicated.\n",
    "        - template: \n",
    "            metadata: \n",
    "            name:myapp-pod\n",
    "            labels:\n",
    "                app: myapp\n",
    "            type: front-end\n",
    "        spec:\n",
    "            containers:\n",
    "    name: nginx-container\n",
    "    image: nginx\n",
    "\n",
    "        replicas: 3\n",
    "        selector:\n",
    "            matchLabels: \n",
    "                type: front-end\n",
    "\n",
    "    The selector section helps the replica set identify which pods fall under it. But why would you have to specify what PODs fall under it, if you have provided set contents of the pod-definition file itself in the template? It’s because, replica set can also manage pods that were not created as part of the replica set creation. Say for example, there were pods created before the creation of the ReplicaSet that match the labels specified in the selector, the replica set will also take those pods into consideration when creating the replicas.\n",
    "\n",
    "        To create the replica set;\n",
    "\n",
    "            kubectl create -f replicaset-definition.yml\n",
    "\n",
    "        To view the replica set;\n",
    "\n",
    "            kubectl get replicaset\n",
    "\n",
    "    To view the pods were created by replica set;\n",
    "\n",
    "            kubectl get pods\n",
    "\n",
    "    By providing the labels under the selector section, replica set will know which pods to monitor and set the specific number of replicas all the time.\n",
    "\n",
    "        To update the replica set definition yaml file;\n",
    "\n",
    "        Kubectl replace -f replicaset-definition.yml\n",
    "\n",
    "        To scale up or down the replica set;\n",
    "\n",
    "        kubectl scale --replicas=6 -f replicaset-definition.yml\n",
    "\n",
    "    -f parameter is used to pass the file name.\n",
    "\n",
    "## __Deployments__\n",
    "\n",
    "\n",
    "    The deployment provides us with capabilities to upgrade the underlying instances seamlessly using rolling updates, undo changes, and pause and resume changes to deployments.\n",
    "\n",
    "    We create a yml file similar to ReplicaSet except that kind parameter is Deployment. When we run the deployment, it will create its own replica sets.\n",
    "\n",
    "    In order to create a Kubernetes deployment;\n",
    "\n",
    "    kubectl create -f deployment-definitions.yml --record\n",
    "\n",
    "    Record is used to store the deployment revisions.\n",
    "\n",
    "    Whenever you create a new deployment or upgrade the images in an existing deployment it triggers a Rollout. A rollout is the process of gradually deploying or upgrading your application containers. \n",
    "\n",
    "    To see rollout deployment status;\n",
    "\n",
    "        kubectl rollout status deployment/myapp-deployment\n",
    "\n",
    "    To check deployment history;\n",
    "\n",
    "        kubectl rollout history deployment/myapp-deployment\n",
    "\n",
    "    With this deployment history we can easily revert back to the version that we want.\n",
    "\n",
    "## __Deployment Strategies\n",
    "\n",
    "\n",
    "    There are two types of deployment strategies. One way to upgrade these to a newer version is to destroy all of these and then create newer versions of application instances. The problem with this is that during the deployment period after the older versions are down and before any newer version is up, the application is down and inaccessible to users. This strategy is known as the Recreate strategy.\n",
    "\n",
    "    The second strategy is where we do not destroy all of them at once. Instead we take down the older version and bring up a newer version one by one. This way the application never goes down and the upgrade is seamless. This is called RollingUpdate strategy. \n",
    "\n",
    "    RollingUpdate strategy is the default method.\n",
    "\n",
    "    When we change anything at the yaml file to update the running project we need to update the existing deployment;\n",
    "\n",
    "    kubectl apply -f deployment-definition.yml\n",
    "\n",
    "## __Networking in Kubernetes_\n",
    "\n",
    "\n",
    "    In Docker, the IP address is assigned to the Docker. But in Kubernetes IP address is assigned to POD.  Each POD has its own IP address.\n",
    "\n",
    "    The PODs have the same IP addresses assigned to them and that will lead to IP conflicts in the network. When a kubernetes cluster is setup, Kubernetes does not automatically setup any kind of networking to handle these issues. As a matter of fact, Kubernetes expects us to setup networking to meet certain fundamental requirements. Some of these are that all the containers or PODs in a kubernetes cluster must be able to communicate with one another without having to configure NAT. All nodes must be able to communicate with containers and all containers must be able to communicate with the nodes in the cluster. Kubernetes expects us to setup a networking solution that meets these criteria. \n",
    "\n",
    "    Fortunately, we don’t have to set it up all on our own as there are multiple pre-built solutions available. Some of them are the cisco ACI networks, Cilium, Big Cloud Fabric, Flannel, Vmware NSX-t and Calico. \n",
    "\n",
    "## __Services__\n",
    "\n",
    "    Kubernetes Services enable communication between various components within and outside of the application. Kubernetes Services helps us connect applications together with other applications or users. For example, our application has groups of PODs running various sections, such as a group for serving front-end load to users, another group running back-end processes, and a third group connecting to an external data source. It is Services that enable connectivity between these groups of PODs. Services enable the front-end application to be made available to users, it helps communication between back-end and front-end PODs, and helps in establishing connectivity to an external data source. Thus services enable loose coupling between microservices in our application.\n",
    "\n",
    "    - Services Types\n",
    "\n",
    "        - Service - Node Port\n",
    "\n",
    "            To link the Service to the defined POD;\n",
    "\n",
    "                Service-definition.yml\n",
    "\n",
    "                apiVersion: apps/v1\n",
    "                kind: Service\n",
    "                metadata: \n",
    "                    name:myapp-service\n",
    "\n",
    "                spec: \n",
    "                    Type: NodePort\n",
    "                    Ports:\n",
    "                targetPort: 80\n",
    "                Port: 80\n",
    "                nodePort: 30008\n",
    "                    selector: #selector gathers the label of the POD which we want to connect\n",
    "                            App: myapp\n",
    "                            Type: front-end\n",
    "\n",
    "                Kubectl create -f Service-definition.yml\n",
    "\n",
    "                Kubectl get services"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
